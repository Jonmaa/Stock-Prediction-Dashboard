{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cac881e6",
   "metadata": {},
   "source": [
    "# Step 6: ML Pipeline\n",
    "\n",
    "**Objective:** Create a reproducible, end-to-end machine learning pipeline that automates the entire workflow from raw data to predictions.\n",
    "\n",
    "## Pipeline Components:\n",
    "1. **Data Ingestion** - Load and validate raw stock data\n",
    "2. **Feature Engineering** - Transform raw data into ML-ready features\n",
    "3. **Model Training** - Train models with proper cross-validation\n",
    "4. **Prediction** - Generate predictions for new data\n",
    "5. **Pipeline Persistence** - Save/load complete pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef757a6",
   "metadata": {},
   "source": [
    "## 6.1 Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c76f598a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries loaded\n",
      "üìÅ Pipeline directory: c:\\git\\Data project\\notebooks\\..\\pipeline\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn pipeline components\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Models\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Technical indicators\n",
    "import ta\n",
    "\n",
    "# Paths\n",
    "DATA_RAW = Path('../data/raw')\n",
    "DATA_FEATURES = Path('../data/features')\n",
    "MODELS_DIR = Path('../models')\n",
    "PIPELINE_DIR = Path('../pipeline')\n",
    "PIPELINE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "TICKERS = ['AAPL', 'MSFT', 'NVDA', 'TSLA', 'AMZN', 'META', 'GOOGL']\n",
    "\n",
    "print('‚úÖ Libraries loaded')\n",
    "print(f'üìÅ Pipeline directory: {PIPELINE_DIR.absolute()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581513f5",
   "metadata": {},
   "source": [
    "## 6.2 Custom Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03a41b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TechnicalIndicatorTransformer defined\n"
     ]
    }
   ],
   "source": [
    "class TechnicalIndicatorTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom transformer to add technical indicators to stock data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.feature_names_ = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        df = X.copy() if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        required = ['open', 'high', 'low', 'close', 'volume']\n",
    "        if not all(col in df.columns for col in required):\n",
    "            raise ValueError(f\"Missing required columns: {required}\")\n",
    "        \n",
    "        # Trend Indicators\n",
    "        df['SMA_10'] = ta.trend.sma_indicator(df['close'], window=10)\n",
    "        df['SMA_20'] = ta.trend.sma_indicator(df['close'], window=20)\n",
    "        df['SMA_50'] = ta.trend.sma_indicator(df['close'], window=50)\n",
    "        df['EMA_10'] = ta.trend.ema_indicator(df['close'], window=10)\n",
    "        df['EMA_20'] = ta.trend.ema_indicator(df['close'], window=20)\n",
    "        \n",
    "        # MACD\n",
    "        macd = ta.trend.MACD(df['close'])\n",
    "        df['MACD'] = macd.macd()\n",
    "        df['MACD_Signal'] = macd.macd_signal()\n",
    "        df['MACD_Hist'] = macd.macd_diff()\n",
    "        \n",
    "        # ADX\n",
    "        adx = ta.trend.ADXIndicator(df['high'], df['low'], df['close'])\n",
    "        df['ADX'] = adx.adx()\n",
    "        df['ADX_Pos'] = adx.adx_pos()\n",
    "        df['ADX_Neg'] = adx.adx_neg()\n",
    "        \n",
    "        # Momentum Indicators\n",
    "        df['RSI'] = ta.momentum.rsi(df['close'], window=14)\n",
    "        df['RSI_Fast'] = ta.momentum.rsi(df['close'], window=7)\n",
    "        \n",
    "        stoch = ta.momentum.StochasticOscillator(df['high'], df['low'], df['close'])\n",
    "        df['Stoch_K'] = stoch.stoch()\n",
    "        df['Stoch_D'] = stoch.stoch_signal()\n",
    "        \n",
    "        df['ROC_5'] = ta.momentum.roc(df['close'], window=5)\n",
    "        df['ROC_10'] = ta.momentum.roc(df['close'], window=10)\n",
    "        \n",
    "        # Volatility Indicators\n",
    "        bb = ta.volatility.BollingerBands(df['close'])\n",
    "        df['BB_High'] = bb.bollinger_hband()\n",
    "        df['BB_Low'] = bb.bollinger_lband()\n",
    "        df['BB_Mid'] = bb.bollinger_mavg()\n",
    "        df['BB_Width'] = (df['BB_High'] - df['BB_Low']) / df['BB_Mid']\n",
    "        df['BB_Pct'] = bb.bollinger_pband()\n",
    "        \n",
    "        df['ATR'] = ta.volatility.average_true_range(df['high'], df['low'], df['close'])\n",
    "        \n",
    "        # Volume Indicators\n",
    "        df['OBV'] = ta.volume.on_balance_volume(df['close'], df['volume'])\n",
    "        df['Volume_SMA_20'] = df['volume'].rolling(20).mean()\n",
    "        df['Volume_Ratio'] = df['volume'] / df['Volume_SMA_20']\n",
    "        \n",
    "        self.feature_names_ = df.columns.tolist()\n",
    "        return df\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return self.feature_names_\n",
    "\n",
    "print('‚úÖ TechnicalIndicatorTransformer defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c108cb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LagFeatureTransformer defined\n"
     ]
    }
   ],
   "source": [
    "class LagFeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Add lag features for specified columns.\n",
    "    \"\"\"\n",
    "    def __init__(self, columns=None, lags=[1, 2, 3, 5, 10, 20]):\n",
    "        self.columns = columns or ['Return', 'RSI', 'Volume_Ratio']\n",
    "        self.lags = lags\n",
    "        self.feature_names_ = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        df = X.copy() if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
    "        \n",
    "        # Calculate Return if not present\n",
    "        if 'Return' not in df.columns and 'close' in df.columns:\n",
    "            df['Return'] = df['close'].pct_change()\n",
    "        \n",
    "        # Add lag features\n",
    "        for col in self.columns:\n",
    "            if col in df.columns:\n",
    "                for lag in self.lags:\n",
    "                    df[f'{col}_Lag_{lag}'] = df[col].shift(lag)\n",
    "        \n",
    "        self.feature_names_ = df.columns.tolist()\n",
    "        return df\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return self.feature_names_\n",
    "\n",
    "print('‚úÖ LagFeatureTransformer defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "977125e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RollingStatsTransformer defined\n"
     ]
    }
   ],
   "source": [
    "class RollingStatsTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Add rolling statistics features.\n",
    "    \"\"\"\n",
    "    def __init__(self, windows=[5, 10, 20]):\n",
    "        self.windows = windows\n",
    "        self.feature_names_ = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        df = X.copy() if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
    "        \n",
    "        if 'Return' not in df.columns and 'close' in df.columns:\n",
    "            df['Return'] = df['close'].pct_change()\n",
    "        \n",
    "        for window in self.windows:\n",
    "            # Return statistics\n",
    "            if 'Return' in df.columns:\n",
    "                df[f'Return_Mean_{window}D'] = df['Return'].rolling(window).mean()\n",
    "                df[f'Return_Std_{window}D'] = df['Return'].rolling(window).std()\n",
    "                df[f'Sharpe_{window}D'] = df[f'Return_Mean_{window}D'] / (df[f'Return_Std_{window}D'] + 1e-10)\n",
    "            \n",
    "            # Price statistics\n",
    "            if 'close' in df.columns:\n",
    "                df[f'Volatility_{window}D'] = df['close'].pct_change().rolling(window).std() * np.sqrt(252)\n",
    "                df[f'Price_Range_{window}D'] = (df['high'].rolling(window).max() - df['low'].rolling(window).min()) / df['close']\n",
    "        \n",
    "        self.feature_names_ = df.columns.tolist()\n",
    "        return df\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return self.feature_names_\n",
    "\n",
    "print('‚úÖ RollingStatsTransformer defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb5d5c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TargetTransformer defined\n"
     ]
    }
   ],
   "source": [
    "class TargetTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Create target variable for direction prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, horizon=1):\n",
    "        self.horizon = horizon\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        df = X.copy() if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
    "        \n",
    "        if 'close' in df.columns:\n",
    "            # Future return\n",
    "            df['Future_Return'] = df['close'].pct_change(self.horizon).shift(-self.horizon)\n",
    "            # Direction: 1 if up, 0 if down\n",
    "            df['Target_Direction'] = (df['Future_Return'] > 0).astype(int)\n",
    "        \n",
    "        return df\n",
    "\n",
    "print('‚úÖ TargetTransformer defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19db3f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DataCleaner defined\n"
     ]
    }
   ],
   "source": [
    "class DataCleaner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Clean data by removing NaN values and selecting feature columns.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_cols=None, drop_na=True):\n",
    "        self.feature_cols = feature_cols\n",
    "        self.drop_na = drop_na\n",
    "        self.fitted_cols_ = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        df = X.copy() if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
    "        \n",
    "        if self.feature_cols is None:\n",
    "            # Auto-detect numeric columns, exclude target and date columns\n",
    "            exclude = ['Target_Direction', 'Future_Return', 'date', 'Date', 'ticker', 'Ticker']\n",
    "            self.fitted_cols_ = [col for col in df.select_dtypes(include=[np.number]).columns \n",
    "                                 if col not in exclude]\n",
    "        else:\n",
    "            self.fitted_cols_ = self.feature_cols\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        df = X.copy() if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
    "        \n",
    "        # Select feature columns that exist\n",
    "        available_cols = [col for col in self.fitted_cols_ if col in df.columns]\n",
    "        \n",
    "        if self.drop_na:\n",
    "            df = df.dropna(subset=available_cols)\n",
    "        \n",
    "        return df\n",
    "\n",
    "print('‚úÖ DataCleaner defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bceb32a",
   "metadata": {},
   "source": [
    "## 6.3 Complete Stock Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f28b582d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ StockPredictionPipeline class defined\n"
     ]
    }
   ],
   "source": [
    "class StockPredictionPipeline:\n",
    "    \"\"\"\n",
    "    End-to-end pipeline for stock direction prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ticker, model=None):\n",
    "        self.ticker = ticker\n",
    "        self.model = model or LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            random_state=42,\n",
    "            verbosity=-1\n",
    "        )\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_cols = None\n",
    "        self.is_fitted = False\n",
    "        \n",
    "        # Transformers\n",
    "        self.tech_transformer = TechnicalIndicatorTransformer()\n",
    "        self.lag_transformer = LagFeatureTransformer(\n",
    "            columns=['Return', 'RSI', 'Volume_Ratio'],\n",
    "            lags=[1, 2, 3, 5, 10, 20]\n",
    "        )\n",
    "        self.rolling_transformer = RollingStatsTransformer(windows=[5, 10, 20])\n",
    "        self.target_transformer = TargetTransformer(horizon=1)\n",
    "        \n",
    "    def _engineer_features(self, df):\n",
    "        \"\"\"Apply all feature engineering transformations.\"\"\"\n",
    "        df = self.tech_transformer.transform(df)\n",
    "        df = self.lag_transformer.transform(df)\n",
    "        df = self.rolling_transformer.transform(df)\n",
    "        df = self.target_transformer.transform(df)\n",
    "        return df\n",
    "    \n",
    "    def _prepare_features(self, df):\n",
    "        \"\"\"Prepare feature matrix X and target y.\"\"\"\n",
    "        # Define feature columns (exclude non-feature columns)\n",
    "        exclude_cols = ['open', 'high', 'low', 'close', 'volume', 'adj close',\n",
    "                       'Target_Direction', 'Future_Return', 'date', 'Date', 'ticker', 'Ticker']\n",
    "        \n",
    "        feature_cols = [col for col in df.select_dtypes(include=[np.number]).columns\n",
    "                       if col not in exclude_cols]\n",
    "        \n",
    "        return feature_cols\n",
    "    \n",
    "    def fit(self, df, verbose=True):\n",
    "        \"\"\"\n",
    "        Fit the complete pipeline on training data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            Raw stock data with OHLCV columns\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"üîß Fitting pipeline for {self.ticker}...\")\n",
    "        \n",
    "        # 1. Feature engineering\n",
    "        df_features = self._engineer_features(df.copy())\n",
    "        \n",
    "        # 2. Get feature columns\n",
    "        self.feature_cols = self._prepare_features(df_features)\n",
    "        \n",
    "        # 3. Drop NaN rows\n",
    "        df_clean = df_features.dropna(subset=self.feature_cols + ['Target_Direction'])\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   Features: {len(self.feature_cols)}\")\n",
    "            print(f\"   Samples: {len(df_clean)}\")\n",
    "        \n",
    "        # 4. Prepare X and y\n",
    "        X = df_clean[self.feature_cols].values\n",
    "        y = df_clean['Target_Direction'].values\n",
    "        \n",
    "        # 5. Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # 6. Train model\n",
    "        self.model.fit(X_scaled, y)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"‚úÖ Pipeline fitted for {self.ticker}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, df):\n",
    "        \"\"\"\n",
    "        Generate predictions for new data.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame with predictions\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Pipeline not fitted. Call fit() first.\")\n",
    "        \n",
    "        # Feature engineering\n",
    "        df_features = self._engineer_features(df.copy())\n",
    "        \n",
    "        # Get available feature columns\n",
    "        available_cols = [col for col in self.feature_cols if col in df_features.columns]\n",
    "        \n",
    "        # Handle missing columns\n",
    "        for col in self.feature_cols:\n",
    "            if col not in df_features.columns:\n",
    "                df_features[col] = 0\n",
    "        \n",
    "        # Prepare features\n",
    "        X = df_features[self.feature_cols].values\n",
    "        \n",
    "        # Handle NaN for prediction\n",
    "        X = np.nan_to_num(X, nan=0)\n",
    "        \n",
    "        # Scale\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        # Predict\n",
    "        predictions = self.model.predict(X_scaled)\n",
    "        probabilities = self.model.predict_proba(X_scaled)[:, 1]\n",
    "        \n",
    "        # Create result DataFrame\n",
    "        result = df_features[['close']].copy()\n",
    "        result['Prediction'] = predictions\n",
    "        result['Probability_Up'] = probabilities\n",
    "        result['Signal'] = np.where(probabilities > 0.5, 'BUY', 'SELL')\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save pipeline to disk.\"\"\"\n",
    "        path = Path(path)\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save components\n",
    "        joblib.dump(self.model, path / f'{self.ticker}_model.pkl')\n",
    "        joblib.dump(self.scaler, path / f'{self.ticker}_scaler.pkl')\n",
    "        \n",
    "        # Save feature columns\n",
    "        with open(path / f'{self.ticker}_features.json', 'w') as f:\n",
    "            json.dump(self.feature_cols, f)\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'ticker': self.ticker,\n",
    "            'n_features': len(self.feature_cols),\n",
    "            'is_fitted': self.is_fitted,\n",
    "            'saved_at': datetime.now().isoformat()\n",
    "        }\n",
    "        with open(path / f'{self.ticker}_metadata.json', 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"üíæ Pipeline saved: {path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path, ticker):\n",
    "        \"\"\"Load pipeline from disk.\"\"\"\n",
    "        path = Path(path)\n",
    "        \n",
    "        pipeline = cls(ticker)\n",
    "        pipeline.model = joblib.load(path / f'{ticker}_model.pkl')\n",
    "        pipeline.scaler = joblib.load(path / f'{ticker}_scaler.pkl')\n",
    "        \n",
    "        with open(path / f'{ticker}_features.json', 'r') as f:\n",
    "            pipeline.feature_cols = json.load(f)\n",
    "        \n",
    "        pipeline.is_fitted = True\n",
    "        \n",
    "        print(f\"üìÇ Pipeline loaded: {ticker}\")\n",
    "        return pipeline\n",
    "\n",
    "print('‚úÖ StockPredictionPipeline class defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aec4a2",
   "metadata": {},
   "source": [
    "## 6.4 Train and Save Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4fa1d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ TRAINING PIPELINES FOR ALL MAGNIFICENT 7 STOCKS\n",
      "============================================================\n",
      "üîß Fitting pipeline for AAPL...\n",
      "   Features: 62\n",
      "   Samples: 1972\n",
      "‚úÖ Pipeline fitted for AAPL\n",
      "üíæ Pipeline saved: ..\\pipeline\\AAPL\n",
      "\n",
      "üîß Fitting pipeline for MSFT...\n",
      "   Features: 62\n",
      "   Samples: 1972\n",
      "‚úÖ Pipeline fitted for MSFT\n",
      "üíæ Pipeline saved: ..\\pipeline\\MSFT\n",
      "\n",
      "üîß Fitting pipeline for NVDA...\n",
      "   Features: 62\n",
      "   Samples: 1972\n",
      "‚úÖ Pipeline fitted for NVDA\n",
      "üíæ Pipeline saved: ..\\pipeline\\NVDA\n",
      "\n",
      "üîß Fitting pipeline for TSLA...\n",
      "   Features: 62\n",
      "   Samples: 1972\n",
      "‚úÖ Pipeline fitted for TSLA\n",
      "üíæ Pipeline saved: ..\\pipeline\\TSLA\n",
      "\n",
      "üîß Fitting pipeline for AMZN...\n",
      "   Features: 62\n",
      "   Samples: 1972\n",
      "‚úÖ Pipeline fitted for AMZN\n",
      "üíæ Pipeline saved: ..\\pipeline\\AMZN\n",
      "\n",
      "üîß Fitting pipeline for META...\n",
      "   Features: 62\n",
      "   Samples: 1972\n",
      "‚úÖ Pipeline fitted for META\n",
      "üíæ Pipeline saved: ..\\pipeline\\META\n",
      "\n",
      "üîß Fitting pipeline for GOOGL...\n",
      "   Features: 62\n",
      "   Samples: 1972\n",
      "‚úÖ Pipeline fitted for GOOGL\n",
      "üíæ Pipeline saved: ..\\pipeline\\GOOGL\n",
      "\n",
      "============================================================\n",
      "‚úÖ All pipelines trained and saved!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Train pipelines for all stocks\n",
    "pipelines = {}\n",
    "\n",
    "print('=' * 60)\n",
    "print('üöÄ TRAINING PIPELINES FOR ALL MAGNIFICENT 7 STOCKS')\n",
    "print('=' * 60)\n",
    "\n",
    "for ticker in TICKERS:\n",
    "    # Load raw data (files have _raw.csv suffix)\n",
    "    file_path = DATA_RAW / f'{ticker}_raw.csv'\n",
    "    df = pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    # Create and fit pipeline\n",
    "    pipeline = StockPredictionPipeline(ticker)\n",
    "    pipeline.fit(df)\n",
    "    \n",
    "    # Save pipeline\n",
    "    pipeline.save(PIPELINE_DIR / ticker)\n",
    "    \n",
    "    pipelines[ticker] = pipeline\n",
    "    print()\n",
    "\n",
    "print('=' * 60)\n",
    "print('‚úÖ All pipelines trained and saved!')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf15a5c",
   "metadata": {},
   "source": [
    "## 6.5 Test Pipeline Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52989ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä TESTING PIPELINE PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "AAPL - Last 5 Predictions:\n",
      "                 close  Probability_Up Signal\n",
      "Date                                         \n",
      "2026-01-09  259.369995        0.807934    BUY\n",
      "2026-01-12  260.250000        0.666508    BUY\n",
      "2026-01-13  261.049988        0.295457   SELL\n",
      "2026-01-14  259.959991        0.142489   SELL\n",
      "2026-01-15  258.209991        0.161919   SELL\n",
      "Signal Distribution: {'BUY': np.int64(1161), 'SELL': np.int64(860)}\n",
      "\n",
      "MSFT - Last 5 Predictions:\n",
      "                 close  Probability_Up Signal\n",
      "Date                                         \n",
      "2026-01-09  479.279999        0.242528   SELL\n",
      "2026-01-12  477.179993        0.233417   SELL\n",
      "2026-01-13  470.670013        0.349091   SELL\n",
      "2026-01-14  459.380005        0.430727   SELL\n",
      "2026-01-15  456.660004        0.427669   SELL\n",
      "Signal Distribution: {'BUY': np.int64(1130), 'SELL': np.int64(891)}\n",
      "\n",
      "NVDA - Last 5 Predictions:\n",
      "                 close  Probability_Up Signal\n",
      "Date                                         \n",
      "2026-01-09  184.860001        0.710141    BUY\n",
      "2026-01-12  184.940002        0.509506    BUY\n",
      "2026-01-13  185.809998        0.517932    BUY\n",
      "2026-01-14  183.139999        0.719871    BUY\n",
      "2026-01-15  187.050003        0.326198   SELL\n",
      "Signal Distribution: {'BUY': np.int64(1129), 'SELL': np.int64(892)}\n"
     ]
    }
   ],
   "source": [
    "# Test predictions with most recent data\n",
    "print('=' * 60)\n",
    "print('üìä TESTING PIPELINE PREDICTIONS')\n",
    "print('=' * 60)\n",
    "\n",
    "for ticker in TICKERS[:3]:  # Test first 3\n",
    "    # Load raw data\n",
    "    file_path = DATA_RAW / f'{ticker}_raw.csv'\n",
    "    df = pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = pipelines[ticker].predict(df)\n",
    "    \n",
    "    print(f\"\\n{ticker} - Last 5 Predictions:\")\n",
    "    print(predictions[['close', 'Probability_Up', 'Signal']].tail())\n",
    "    \n",
    "    # Signal distribution\n",
    "    signal_counts = predictions['Signal'].value_counts()\n",
    "    print(f\"Signal Distribution: {dict(signal_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c98ae80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîÑ TESTING PIPELINE LOADING\n",
      "============================================================\n",
      "üìÇ Pipeline loaded: AAPL\n",
      "\n",
      "Loaded AAPL Pipeline - Last 5 Predictions:\n",
      "                 close  Probability_Up Signal\n",
      "Date                                         \n",
      "2026-01-09  259.369995        0.807934    BUY\n",
      "2026-01-12  260.250000        0.666508    BUY\n",
      "2026-01-13  261.049988        0.295457   SELL\n",
      "2026-01-14  259.959991        0.142489   SELL\n",
      "2026-01-15  258.209991        0.161919   SELL\n"
     ]
    }
   ],
   "source": [
    "# Test loading pipeline from disk\n",
    "print('=' * 60)\n",
    "print('üîÑ TESTING PIPELINE LOADING')\n",
    "print('=' * 60)\n",
    "\n",
    "# Load a saved pipeline\n",
    "loaded_pipeline = StockPredictionPipeline.load(PIPELINE_DIR / 'AAPL', 'AAPL')\n",
    "\n",
    "# Test prediction with loaded pipeline\n",
    "df_test = pd.read_csv(DATA_RAW / 'AAPL_raw.csv', index_col=0, parse_dates=True)\n",
    "df_test.columns = df_test.columns.str.lower()\n",
    "\n",
    "predictions = loaded_pipeline.predict(df_test)\n",
    "print(f\"\\nLoaded AAPL Pipeline - Last 5 Predictions:\")\n",
    "print(predictions[['close', 'Probability_Up', 'Signal']].tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4b945a",
   "metadata": {},
   "source": [
    "## 6.6 Batch Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b12c67eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä LATEST PREDICTIONS FOR ALL MAGNIFICENT 7\n",
      "============================================================\n",
      "üìÇ Pipeline loaded: AAPL\n",
      "üìÇ Pipeline loaded: MSFT\n",
      "üìÇ Pipeline loaded: NVDA\n",
      "üìÇ Pipeline loaded: TSLA\n",
      "üìÇ Pipeline loaded: AMZN\n",
      "üìÇ Pipeline loaded: META\n",
      "üìÇ Pipeline loaded: GOOGL\n",
      "\n",
      "Ticker       Date      Close  Prob_Up Signal\n",
      "  AAPL 2026-01-15 258.209991 0.161919   SELL\n",
      "  MSFT 2026-01-15 456.660004 0.427669   SELL\n",
      "  NVDA 2026-01-15 187.050003 0.326198   SELL\n",
      "  TSLA 2026-01-15 438.570007 0.245648   SELL\n",
      "  AMZN 2026-01-15 238.179993 0.178959   SELL\n",
      "  META 2026-01-15 620.799988 0.293636   SELL\n",
      " GOOGL 2026-01-15 332.779999 0.417860   SELL\n"
     ]
    }
   ],
   "source": [
    "def get_all_predictions(pipeline_dir, data_dir, tickers):\n",
    "    \"\"\"\n",
    "    Generate predictions for all stocks.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame with latest predictions for all stocks\n",
    "    \"\"\"\n",
    "    all_predictions = []\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        # Load pipeline\n",
    "        pipeline = StockPredictionPipeline.load(pipeline_dir / ticker, ticker)\n",
    "        \n",
    "        # Load data\n",
    "        df = pd.read_csv(data_dir / f'{ticker}_raw.csv', index_col=0, parse_dates=True)\n",
    "        df.columns = df.columns.str.lower()\n",
    "        \n",
    "        # Get predictions\n",
    "        pred = pipeline.predict(df)\n",
    "        \n",
    "        # Get latest prediction\n",
    "        latest = pred.iloc[-1].copy()\n",
    "        latest['Ticker'] = ticker\n",
    "        latest['Date'] = pred.index[-1]\n",
    "        \n",
    "        all_predictions.append(latest)\n",
    "    \n",
    "    result = pd.DataFrame(all_predictions)\n",
    "    result = result[['Ticker', 'Date', 'close', 'Probability_Up', 'Signal']]\n",
    "    result.columns = ['Ticker', 'Date', 'Close', 'Prob_Up', 'Signal']\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Get all predictions\n",
    "print('=' * 60)\n",
    "print('üìä LATEST PREDICTIONS FOR ALL MAGNIFICENT 7')\n",
    "print('=' * 60)\n",
    "\n",
    "all_preds = get_all_predictions(PIPELINE_DIR, DATA_RAW, TICKERS)\n",
    "print()\n",
    "print(all_preds.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de5bc6",
   "metadata": {},
   "source": [
    "## 6.7 Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b034c4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üè≠ PIPELINE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "üì¶ PIPELINE COMPONENTS:\n",
      "   1. TechnicalIndicatorTransformer - Adds 25+ technical indicators\n",
      "   2. LagFeatureTransformer - Creates lag features for key indicators\n",
      "   3. RollingStatsTransformer - Adds rolling statistics\n",
      "   4. TargetTransformer - Creates direction target variable\n",
      "   5. StandardScaler - Normalizes features\n",
      "   6. LGBMClassifier - Predicts direction\n",
      "\n",
      "üìÅ SAVED FILES:\n",
      "   AAPL/: 4 files\n",
      "   MSFT/: 4 files\n",
      "   NVDA/: 4 files\n",
      "   TSLA/: 4 files\n",
      "   AMZN/: 4 files\n",
      "   META/: 4 files\n",
      "   GOOGL/: 4 files\n",
      "\n",
      "üîß PIPELINE USAGE:\n",
      "   # Load pipeline\n",
      "   pipeline = StockPredictionPipeline.load(path, ticker)\n",
      "   \n",
      "   # Get predictions\n",
      "   predictions = pipeline.predict(new_data)\n",
      "\n",
      "======================================================================\n",
      "‚úÖ STEP 6 COMPLETE - Ready for 07_validation.ipynb\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print('=' * 70)\n",
    "print('üè≠ PIPELINE SUMMARY')\n",
    "print('=' * 70)\n",
    "\n",
    "print(\"\\nüì¶ PIPELINE COMPONENTS:\")\n",
    "print(\"   1. TechnicalIndicatorTransformer - Adds 25+ technical indicators\")\n",
    "print(\"   2. LagFeatureTransformer - Creates lag features for key indicators\")\n",
    "print(\"   3. RollingStatsTransformer - Adds rolling statistics\")\n",
    "print(\"   4. TargetTransformer - Creates direction target variable\")\n",
    "print(\"   5. StandardScaler - Normalizes features\")\n",
    "print(\"   6. LGBMClassifier - Predicts direction\")\n",
    "\n",
    "print(\"\\nüìÅ SAVED FILES:\")\n",
    "for ticker in TICKERS:\n",
    "    ticker_dir = PIPELINE_DIR / ticker\n",
    "    if ticker_dir.exists():\n",
    "        files = list(ticker_dir.glob('*'))\n",
    "        print(f\"   {ticker}/: {len(files)} files\")\n",
    "\n",
    "print(\"\\nüîß PIPELINE USAGE:\")\n",
    "print(\"   # Load pipeline\")\n",
    "print(\"   pipeline = StockPredictionPipeline.load(path, ticker)\")\n",
    "print(\"   \")\n",
    "print(\"   # Get predictions\")\n",
    "print(\"   predictions = pipeline.predict(new_data)\")\n",
    "\n",
    "print(\"\\n\" + '=' * 70)\n",
    "print('‚úÖ STEP 6 COMPLETE - Ready for 07_validation.ipynb')\n",
    "print('=' * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
